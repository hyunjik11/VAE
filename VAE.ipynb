{
 "metadata": {
  "name": "",
  "signature": "sha256:4cb97705adfce1a230f58e7e06e27dd8ff16a5fcef27505c274c88d680b8fb32"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
        "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
        "Successfully downloaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " train-labels-idx1-ubyte.gz 28881 bytes.\n",
        "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
        "Successfully downloaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
        "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
        "Successfully downloaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
        "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "np.random.seed(0)\n",
      "tf.set_random_seed(0)\n",
      "from tensorflow.examples.tutorials.mnist import input_data\n",
      "mnist = input_data.read_data_sets(\"MNIST_data\")\n",
      "n_train_samples = mnist.train.num_examples\n",
      "n_valid_samples = mnist.validation.num_examples\n",
      "n_samples = n_train_samples + n_valid_samples\n",
      "# using images with pixel values in {0,1}. i.e. p(x|z) is bernoulli"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
        "Extracting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " MNIST_data/train-labels-idx1-ubyte.gz\n",
        "Extracting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " MNIST_data/t10k-images-idx3-ubyte.gz\n",
        "Extracting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " MNIST_data/t10k-labels-idx1-ubyte.gz\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def xavier_init(fan_in, fan_out, constant=1): \n",
      "    \"\"\" Xavier initialization of network weights\"\"\"\n",
      "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
      "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
      "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
      "    return tf.random_uniform((fan_in, fan_out), \n",
      "                             minval=low, maxval=high, \n",
      "                             dtype=tf.float32)\n",
      "\n",
      "def bernoullisample(x):\n",
      "    return np.random.binomial(1,x,size=x.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_train_samples = mnist.train.num_examples\n",
      "n_valid_samples = mnist.validation.num_examples\n",
      "n_test_samples = mnist.test.num_examples\n",
      "n_samples = n_train_samples + n_valid_samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class VariationalAutoencoder(object):\n",
      "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
      "    \n",
      "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
      "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
      "    end-to-end.\n",
      "    \n",
      "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
      "    \"\"\"\n",
      "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
      "                 learning_rate=0.001, batch_size=100):\n",
      "        self.network_architecture = network_architecture\n",
      "        self.transfer_fct = transfer_fct\n",
      "        self.learning_rate = learning_rate\n",
      "        self.batch_size = batch_size\n",
      "        \n",
      "        # tf Graph input (placeholder objects can only be used as input, and can't be evaluated)\n",
      "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
      "        \n",
      "        # Create autoencoder network\n",
      "        self._create_network()\n",
      "        # Define loss function based variational upper-bound and \n",
      "        # corresponding optimizer\n",
      "        self._create_loss_optimizer()\n",
      "        \n",
      "        # Initializing the tensorflow variables\n",
      "        init = tf.initialize_all_variables()\n",
      "\n",
      "        # Launch the session\n",
      "        self.sess = tf.InteractiveSession()\n",
      "        self.sess.run(init) # initialize all tensorflow variables\n",
      "    \n",
      "    def _create_network(self):\n",
      "        # Initialize autoencode network weights and biases\n",
      "        network_weights = self._initialize_weights(**self.network_architecture)\n",
      "        # this feeds in the values of self.network_architecture as arguments to _initialize_weights\n",
      "        \n",
      "        # Use recognition network to determine mean and \n",
      "        # (log) variance of Gaussian distribution in latent\n",
      "        # space\n",
      "        self.z_mean, self.z_log_sigma_sq = \\\n",
      "            self._recognition_network(network_weights[\"weights_recog\"], \n",
      "                                      network_weights[\"biases_recog\"])\n",
      "\n",
      "        # Draw one sample z from Gaussian distribution\n",
      "        # n_z is dimensionality of latent space\n",
      "        n_z = self.network_architecture[\"n_z\"]\n",
      "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
      "                               dtype=tf.float32)\n",
      "        # z = mu + sigma*epsilon\n",
      "        self.z = tf.add(self.z_mean, \n",
      "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
      "\n",
      "        # Use generator to determine mean of\n",
      "        # Bernoulli distribution of reconstructed input\n",
      "        self.x_reconstr_mean = \\\n",
      "            self._generator_network(network_weights[\"weights_gener\"],\n",
      "                                    network_weights[\"biases_gener\"])\n",
      "            \n",
      "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
      "                            n_hidden_gener_1,  n_hidden_gener_2, \n",
      "                            n_input, n_z):\n",
      "        # initialize weights, stored in dictionary all_weights (member of VAE class)\n",
      "        # these are all the variables that will be learned\n",
      "        # Use Xaiver init for weights, 0 for biases\n",
      "        all_weights = dict()\n",
      "        all_weights['weights_recog'] = {\n",
      "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
      "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
      "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
      "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
      "        all_weights['biases_recog'] = {\n",
      "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
      "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
      "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
      "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
      "        all_weights['weights_gener'] = {\n",
      "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
      "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
      "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
      "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
      "        all_weights['biases_gener'] = {\n",
      "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
      "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
      "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
      "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
      "        return all_weights\n",
      "            \n",
      "    def _recognition_network(self, weights, biases):\n",
      "        # Generate probabilistic encoder (recognition network), which\n",
      "        # maps inputs onto mean and log_sigma_sq vector of a normal distribution in latent space.\n",
      "        # The transformation is parametrized and can be learned.\n",
      "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
      "                                           biases['b1'])) \n",
      "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
      "                                           biases['b2'])) \n",
      "        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
      "                        biases['out_mean']) # \n",
      "        z_log_sigma_sq = \\\n",
      "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
      "                   biases['out_log_sigma'])\n",
      "        return (z_mean, z_log_sigma_sq)\n",
      "\n",
      "    def _generator_network(self, weights, biases):\n",
      "        # Generate probabilistic decoder (decoder network), which\n",
      "        # maps points in latent space onto mean param of Bernoulli distribution in data space.\n",
      "        # The transformation is parametrized and can be learned.\n",
      "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
      "                                           biases['b1'])) \n",
      "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
      "                                           biases['b2'])) \n",
      "        x_reconstr_mean = \\\n",
      "            tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']), \n",
      "                                 biases['out_mean']))\n",
      "        return x_reconstr_mean\n",
      "            \n",
      "    def _create_loss_optimizer(self):\n",
      "        # The loss is composed of two terms:\n",
      "        # 1.) The reconstruction loss (the negative log probability\n",
      "        #     of the input under the reconstructed Bernoulli distribution \n",
      "        #     induced by the decoder in the data space).\n",
      "        #     This can be interpreted as the number of \"nats\" required\n",
      "        #     for reconstructing the input when the activation in latent\n",
      "        #     is given.\n",
      "        # Adding 1e-10 to avoid evaluatio of log(0.0)\n",
      "        reconstr_loss = \\\n",
      "            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
      "                           + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n",
      "                           1)\n",
      "        # reduce_sum(x,1) is summing the rows of x (i.e. summing across input dim)\n",
      "        # * is elementwise multiplication\n",
      "        \n",
      "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
      "        ##    between the distribution in latent space induced by the encoder on \n",
      "        #     the data and some prior. This acts as a kind of regularizer.\n",
      "        #     This can be interpreted as the number of \"nats\" required\n",
      "        #     for transmitting the the latent space distribution given\n",
      "        #     the prior.\n",
      "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
      "                                           - tf.square(self.z_mean) \n",
      "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
      "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
      "        # Use ADAM optimizer\n",
      "        self.optimizer = \\\n",
      "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
      "        \n",
      "    def partial_fit(self, X):\n",
      "        \"\"\"Train model based on mini-batch of input data.\n",
      "        \n",
      "        Return cost of mini-batch.\n",
      "        \"\"\"\n",
      "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
      "                                  feed_dict={self.x: X})\n",
      "        return cost\n",
      "    \n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
      "        # Note: This maps to mean of distribution, we could alternatively\n",
      "        # sample from Gaussian distribution\n",
      "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
      "    \n",
      "    def generate(self, z_mu=None):\n",
      "        \"\"\" Generate data by sampling from latent space.\n",
      "        \n",
      "        If z_mu is not None, data for this point in latent space is\n",
      "        generated. Otherwise, z_mu is drawn from prior in latent \n",
      "        space.        \n",
      "        \"\"\"\n",
      "        if z_mu is None:\n",
      "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
      "        # Note: This maps to mean of distribution, we could alternatively\n",
      "        # sample from Gaussian(Bernoulli?) distribution\n",
      "        return self.sess.run(self.x_reconstr_mean, \n",
      "                             feed_dict={self.z: z_mu})\n",
      "    \n",
      "    def reconstruct(self, X):\n",
      "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
      "        return self.sess.run(self.x_reconstr_mean, \n",
      "                             feed_dict={self.x: X})\n",
      "    \n",
      "    def test_cost(self, X):\n",
      "        \"\"\" Return cost of mini-batch without further training. \"\"\"\n",
      "        return self.sess.run(self.cost,feed_dict={self.x: X})\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train(network_architecture, learning_rate=0.001,\n",
      "          batch_size=100, training_epochs=10, display_step=5, transfer_fct=tf.nn.softplus):\n",
      "    vae = VariationalAutoencoder(network_architecture, \n",
      "                                 learning_rate=learning_rate, \n",
      "                                 batch_size=batch_size,transfer_fct=transfer_fct)\n",
      "    # Training cycle\n",
      "    np.random.seed(0)\n",
      "    for epoch in range(training_epochs):\n",
      "        avg_cost = 0.\n",
      "        testcost=0.\n",
      "        total_train_batch = int(n_train_samples / batch_size)\n",
      "        # total_valid_batch = int(n_valid_samples / batch_size)\n",
      "        total_test_batch = int(n_test_samples / batch_size)\n",
      "        # total_batch = total_train_batch + total_valid_batch\n",
      "        # Loop over all batches\n",
      "        for i in range(total_train_batch):\n",
      "            #if i < total_train_batch:\n",
      "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
      "            #else:\n",
      "            #    batch_xs, _ = mnist.validation.next_batch(batch_size)\n",
      "            # Fit training using batch data\n",
      "            batch_xs = bernoullisample(batch_xs)\n",
      "            \n",
      "            cost = vae.partial_fit(batch_xs)\n",
      "            # Compute average training ELBO\n",
      "            avg_cost += cost / n_train_samples * batch_size\n",
      "\n",
      "        # Display logs per epoch step\n",
      "        if epoch % display_step == 0:\n",
      "            for i in range(total_test_batch):\n",
      "                batch_xs, _ = mnist.test.next_batch(batch_size)\n",
      "                batch_xs = bernoullisample(batch_xs)\n",
      "                testcost += vae.test_cost(batch_xs) / n_test_samples * batch_size\n",
      "                \n",
      "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
      "                  \"trainELBO=\", \"{:.9f}\".format(avg_cost), \\\n",
      "                  \"testELBO=\", \"{:.9f}\".format(testcost)\n",
      "    return vae"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "network_architecture = \\\n",
      "    dict(n_hidden_recog_1=500, # 1st layer encoder neurons\n",
      "         n_hidden_recog_2=500, # 2nd layer encoder neurons\n",
      "         n_hidden_gener_1=500, # 1st layer decoder neurons\n",
      "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
      "         n_input=784, # MNIST data input (img shape: 28*28)\n",
      "         n_z=20)  # dimensionality of latent space\n",
      "\n",
      "#CUDA_VISIBLE_DEVICES=1,2,4,6,7\n",
      "vae = train(network_architecture, training_epochs=75, display_step=1, transfer_fct=tf.nn.relu)\n",
      "vae.sess.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Epoch: 0001 trainELBO= 160.922406450 testELBO= 125.197066193\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0002 trainELBO= 119.323297161 testELBO= 113.340956268\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0003 trainELBO= 111.913053658 testELBO= 108.714474182\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0004 trainELBO= 108.247309043 testELBO= 105.561964035\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0005 trainELBO= 105.845002483 testELBO= 104.522814941\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0006 trainELBO= 104.156037487 testELBO= 102.623133392\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0007 trainELBO= 103.026265911 testELBO= 102.586474838\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0008 trainELBO= 102.023589075 testELBO= 101.399603500\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0009 trainELBO= 101.330018241 testELBO= 100.328753738\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0010 trainELBO= 100.644143261 testELBO= 99.920440445\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0011 trainELBO= 100.067170147 testELBO= 99.359169312\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0012 trainELBO= 99.656893005 testELBO= 99.631012421\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0013 trainELBO= 99.100147400 testELBO= 99.118402328\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0014 trainELBO= 98.916745966 testELBO= 98.596040497\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0015 trainELBO= 98.490303678 testELBO= 98.591734314\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0016 trainELBO= 98.197265403 testELBO= 98.315239182\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0017 trainELBO= 97.873388727 testELBO= 97.741419220\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0018 trainELBO= 97.707183838 testELBO= 98.306057816\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0019 trainELBO= 97.391585971 testELBO= 97.460512619\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0020 trainELBO= 97.217497767 testELBO= 97.349420166\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0021 trainELBO= 97.055256847 testELBO= 97.416623077\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0022 trainELBO= 96.816225891 testELBO= 97.163257904\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0023 trainELBO= 96.630086087 testELBO= 96.632190933\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0024 trainELBO= 96.533147597 testELBO= 96.962391663\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0025 trainELBO= 96.371169767 testELBO= 96.861851349\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0026 trainELBO= 96.202164307 testELBO= 96.921572495\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0027 trainELBO= 96.177171839 testELBO= 96.418015976\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0028 trainELBO= 96.006986625 testELBO= 96.966625366\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0029 trainELBO= 95.913439498 testELBO= 96.993840408\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0030 trainELBO= 95.744417641 testELBO= 95.999828720\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0031 trainELBO= 95.648427845 testELBO= 96.534806213\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0032 trainELBO= 95.539437922 testELBO= 96.030588379\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0033 trainELBO= 95.477461978 testELBO= 96.566236649\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0034 trainELBO= 95.309267634 testELBO= 95.891926346\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0035 trainELBO= 95.276470059 testELBO= 95.975816879\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0036 trainELBO= 95.130697562 testELBO= 95.437095032\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0037 trainELBO= 95.116886375 testELBO= 95.804605255\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0038 trainELBO= 95.049232386 testELBO= 95.898151474\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0039 trainELBO= 94.895715804 testELBO= 95.900745087\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0040 trainELBO= 94.792456415 testELBO= 95.647835007\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0041 trainELBO= 94.813269182 testELBO= 96.031444626\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0042 trainELBO= 94.709861422 testELBO= 95.667030182\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0043 trainELBO= 94.615394717 testELBO= 96.020214310\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0044 trainELBO= 94.623632688 testELBO= 95.799743500\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0045 trainELBO= 94.427196919 testELBO= 96.006585541\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0046 trainELBO= 94.431641388 testELBO= 95.972314606\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0047 trainELBO= 94.428648515 testELBO= 95.433791504\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0048 trainELBO= 94.283012196 testELBO= 95.133344574\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0049 trainELBO= 94.283636322 testELBO= 95.567787247\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0050 trainELBO= 94.293714059 testELBO= 95.565174637\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0051 trainELBO= 94.222538272 testELBO= 95.212135162\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0052 trainELBO= 94.158298839 testELBO= 94.987314911\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0053 trainELBO= 94.070720797 testELBO= 95.459965134\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0054 trainELBO= 94.044169006 testELBO= 95.416017303\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0055 trainELBO= 94.049701080 testELBO= 95.827649460\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0056 trainELBO= 93.994052346 testELBO= 95.434298096\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0057 trainELBO= 93.978146487 testELBO= 94.928582077\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0058 trainELBO= 93.882268760 testELBO= 95.165872803\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0059 trainELBO= 93.838866008 testELBO= 95.452836151\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0060 trainELBO= 93.773867659 testELBO= 95.043041534\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0061 trainELBO= 93.730269748 testELBO= 95.640752716\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0062 trainELBO= 93.697840909 testELBO= 94.989317017\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0063 trainELBO= 93.718670238 testELBO= 95.201872101\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0064 trainELBO= 93.732403911 testELBO= 94.931203918\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0065 trainELBO= 93.656533134 testELBO= 95.523574753\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0066 trainELBO= 93.611345756 testELBO= 94.736627274\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0067 trainELBO= 93.499466761 testELBO= 95.143111343\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0068 trainELBO= 93.522571841 testELBO= 94.818554687\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0069 trainELBO= 93.408254644 testELBO= 94.989852142\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0070 trainELBO= 93.468495802 testELBO= 95.083751297\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0071 trainELBO= 93.428532895 testELBO= 94.348250427\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0072 trainELBO= 93.380632213 testELBO= 95.186938705\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0073 trainELBO= 93.316523562 testELBO= 94.928077850\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0074 trainELBO= 93.279828158 testELBO= 95.054427338\n",
        "Epoch:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0075 trainELBO= 93.250678017 testELBO= 94.600400848\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "network_architecture[\"n_z\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 114,
       "text": [
        "20"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_sample = mnist.test.next_batch(100)[0]\n",
      "x_reconstruct = vae.reconstruct(x_sample)\n",
      "\n",
      "plt.figure(figsize=(8, 12))\n",
      "for i in range(5):\n",
      "\n",
      "    plt.subplot(5, 2, 2*i + 1)\n",
      "    plt.imshow(x_sample[i].reshape(28, 28), vmin=0, vmax=1)\n",
      "    plt.title(\"Test input\")\n",
      "    plt.colorbar()\n",
      "    plt.subplot(5, 2, 2*i + 2)\n",
      "    plt.imshow(x_reconstruct[i].reshape(28, 28), vmin=0, vmax=1)\n",
      "    plt.title(\"Reconstruction\")\n",
      "    plt.colorbar()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-89-9341ebeeb2c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a=np.empty(4,dtype=int);\n",
      "idx=tf.Variable(a,dtype=tf.int32)\n",
      "init = tf.initialize_all_variables()\n",
      "sess=tf.InteractiveSession()\n",
      "sess.run(init)\n",
      "print sess.run(idx)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 0 0 0]\n"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[4 5 6 0]\n"
       ]
      }
     ],
     "prompt_number": 148
    }
   ],
   "metadata": {}
  }
 ]
}