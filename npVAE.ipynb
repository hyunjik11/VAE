{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\")\n",
    "n_train_samples = mnist.train.num_examples\n",
    "n_valid_samples = mnist.validation.num_examples\n",
    "n_total_samples = n_train_samples + n_valid_samples\n",
    "# using images with pixel values in {0,1}. i.e. p(x|z) is bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "\n",
    "def gauss_init(n_params,stddev=1): #initialise params by Gaussians\n",
    "    return tf.random_normal([n_params],stddev=stddev,dtype=tf.float32)\n",
    "\n",
    "def convert_col_idx(n_z,idx): \n",
    "# convert col indices of 2D array with n_z rows to indices for flattened vector\n",
    "    ncol=len(idx)\n",
    "    indices=np.empty(n_z*ncol,dtype=int)\n",
    "    for i in xrange(0,ncol):\n",
    "        indices[i*n_z:(i+1)*n_z]=np.arange(idx[i]*n_z,(idx[i]+1)*n_z)\n",
    "    return indices\n",
    "\n",
    "def bernoullisample(x):\n",
    "    return np.random.binomial(1,x,size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze_2:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.placeholder(tf.int32, [3])\n",
    "tf.shape(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "    \n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "    \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # tf Graph input (placeholder objects can only be used as input, and can't be evaluated)\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        \n",
    "        # placeholder object for indices of params in recog net\n",
    "        # corresponding to  minibatch (use 0 indexing)\n",
    "        self.idx = tf.placeholder(tf.int32, [batch_size*network_architecture[\"n_z\"]])\n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init) #run __init__ function\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        self._initialize_weights(**self.network_architecture)\n",
    "        # this feeds in the values of self.network_architecture as arguments to _initialize_weights\n",
    "        \n",
    "        # Use recognition network to determine mean and \n",
    "        # (log) variance of Gaussian distribution in latent\n",
    "        # space\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        n_samples = self.network_architecture[\"n_samples\"]\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self._recognition_network(n_z,n_samples)\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        # n_z is dimensionality of latent space\n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        self.z = tf.add(self.z_mean, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        # Use generator to determine mean of\n",
    "        # Bernoulli distribution of reconstructed input\n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network()\n",
    "            \n",
    "    def _initialize_weights(self, n_samples, \n",
    "                            n_hidden_gener_1, n_hidden_gener_2, n_input, n_z):\n",
    "        # initialize weights, stored in dictionary all_weights (member of VAE class)\n",
    "        # these are all the variables that will be learned\n",
    "        # Use Xaiver init for weights, 0 for biases\n",
    "        all_weights = dict()\n",
    "        all_weights['params_recog'] = {\n",
    "            'mu': tf.Variable(gauss_init(n_z*n_samples)),\n",
    "            'log_sigma_sq': tf.Variable(gauss_init(n_z*n_samples))}\n",
    "            # stack each column of params(n_z by n_samples) into one column vector\n",
    "        all_weights['weights_gener'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        self.params=all_weights['params_recog']\n",
    "        self.weights=all_weights['weights_gener']\n",
    "        self.biases=all_weights['biases_gener']\n",
    "            \n",
    "    def _recognition_network(self, n_z, n_samples):\n",
    "        # The approximate posterior distribution, which is a fully factorized Gaussian \n",
    "        # in the latent variables.\n",
    "        # Output mean and log_sigma_sq of z_n for the minibatch given by self.idx\n",
    "        z_mean = tf.reshape(tf.gather(self.params['mu'],self.idx),[-1,n_z])\n",
    "        z_log_sigma_sq = tf.reshape(tf.gather(self.params['log_sigma_sq'],self.idx),[-1,n_z])\n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto mean param of Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, self.weights['h1']), \n",
    "                                           self.biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, self.weights['h2']), \n",
    "                                           self.biases['b2'])) \n",
    "        x_reconstr_mean = \\\n",
    "            tf.nn.sigmoid(tf.add(tf.matmul(layer_2, self.weights['out_mean']), \n",
    "                                 self.biases['out_mean']))\n",
    "        return x_reconstr_mean\n",
    "            \n",
    "    def _create_loss(self):\n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution \n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\n",
    "        # Adding 1e-10 to avoid evaluatio of log(0.0)\n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
    "                           + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n",
    "                           1)\n",
    "        # reduce_sum(x,1) is summing the rows of x (i.e. summing across input dim)\n",
    "        # * is elementwise multiplication\n",
    "        \n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                           - tf.square(self.z_mean) \n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.reconstr = tf.reduce_mean(reconstr_loss)\n",
    "        self.latent = tf.reduce_mean(latent_loss)\n",
    "        self.cost = self.reconstr + self.latent\n",
    "        #self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "    \n",
    "    #def optimize(self):\n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        # optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        # Clip gradients\n",
    "        # gvs = optimizer.compute_gradients(self.cost)\n",
    "        # capped_gvs = [(tf.clip_by_norm(gv[0], Inf), gv[1]) for gv in gvs]\n",
    "        # optimizer.apply_gradients(gvs)\n",
    "        \n",
    "        \n",
    "    def partial_fit(self, X, myidx):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost \\\n",
    "        = self.sess.run((self.optimizer,self.cost), \\\n",
    "                        feed_dict={self.x: X, self.idx: myidx})\n",
    "    \n",
    "        return cost\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian(Bernoulli?) distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X})\n",
    "    \n",
    "    def test_cost(self, X):\n",
    "        \"\"\" Return cost of mini-batch without further training. \"\"\"\n",
    "        return self.sess.run(self.cost,feed_dict={self.x: X})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(network_architecture, learning_rate=0.001,\n",
    "          batch_size=100, training_epochs=10, display_step=5, transfer_fct=tf.nn.softplus):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size,transfer_fct=transfer_fct)\n",
    "    # Training cycle\n",
    "    np.random.seed(0)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.; avg_reconstr = 0.; avg_latent = 0.;\n",
    "        z_mean_max = -1.0; z_sigma_sq_min= 100.0;\n",
    "        testcost=0.;\n",
    "        weights_max = 0;\n",
    "        total_train_batch = int(n_train_samples / batch_size)\n",
    "        # total_valid_batch = int(n_valid_samples / batch_size)\n",
    "        # total_batch = total_train_batch + total_valid_batch\n",
    "        # Loop over all batches\n",
    "        for i in range(total_train_batch):\n",
    "            #if i < total_train_batch:\n",
    "            batch_xs, _ , batch_idx = mnist.train.next_batch(batch_size)\n",
    "            #else:\n",
    "            #    batch_xs, _ = mnist.validation.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            batch_xs = bernoullisample(batch_xs)\n",
    "            idx = convert_col_idx(network_architecture[\"n_z\"],batch_idx) \n",
    "            \n",
    "            cost = vae.partial_fit(batch_xs, idx)\n",
    "            # Compute average training ELBO\n",
    "            # z_mean_max = np.maximum(np.amax(z_mean),z_mean_max)\n",
    "            # z_sigma_sq_min = np.minimum(np.exp(np.amin(z_log_sigma_sq)),z_sigma_sq_min)\n",
    "            #avg_reconstr += reconstr / n_train_samples * batch_size\n",
    "            #avg_latent += latent / n_train_samples * batch_size\n",
    "            \"\"\"weights_max = max(np.amax(np.absolute(weights1)),\n",
    "                              np.amax(np.absolute(weights2)),\n",
    "                              np.amax(np.absolute(weights3)),\n",
    "                              np.amax(np.absolute(weights4)),\n",
    "                              np.amax(np.absolute(weights5)),\n",
    "                              np.amax(np.absolute(weights6)),\n",
    "                              np.amax(np.absolute(weights7)),\n",
    "                              np.amax(np.absolute(weights8)),\n",
    "                              weights_max)\n",
    "                              \"\"\"\n",
    "            avg_cost += cost / n_train_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            #for i in range(total_test_batch):\n",
    "            #    batch_xs, _ = mnist.test.next_batch(batch_size)\n",
    "            #    batch_xs = bernoullisample(batch_xs)\n",
    "            #    testcost += vae.test_cost(batch_xs) / n_test_samples * batch_size\n",
    "                \n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"trainELBO=\", \"{:.3f}\".format(avg_cost)\n",
    "                  #\"weights_max=\", \"{:.4f}\".format(weights_max)\n",
    "                  #\"Reconstr=\", \"{:.3f}\".format(avg_reconstr), \\\n",
    "                  #\"Latent=\", \"{:.3f}\".format(avg_latent), \\\n",
    "                  #\"z_mean_max=\", \"{:.4f}\".format(z_mean_max), \\\n",
    "                  #\"z_sigma_sq_min=\", \"{:.4f}\".format(z_sigma_sq_min)\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 trainELBO= 562.393\n"
     ]
    }
   ],
   "source": [
    "network_architecture = \\\n",
    "    dict(n_samples=n_train_samples, # number of training data points\n",
    "         n_hidden_gener_1=500, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
    "         n_input=784, # MNIST data input (img shape: 28*28)\n",
    "         n_z=20)  # dimensionality of latent space\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=\"\"\n",
    "vae = train(network_architecture, batch_size=100,training_epochs=1, display_step=1, transfer_fct=tf.nn.relu)\n",
    "vae.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=[1,2,3]; \n",
    "perm=np.arange(3); \n",
    "np.random.shuffle(perm); \n",
    "b=[idx[i] for i in perm]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Variable' object is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-e7bbc992e335>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#print sess.run(tf.gather(b,idx))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/homes/hkim/.local/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    370\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0minvoked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[1;32m--> 372\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'Variable' object is not iterable.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Variable' object is not iterable."
     ]
    }
   ],
   "source": [
    "idx=tf.Variable([0,1],dtype=tf.int32)\n",
    "b=tf.Variable([4.0,3.0,2.0],dtype=tf.float32)\n",
    "sess.close()\n",
    "init = tf.initialize_all_variables()\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "c=[[1,2] for i in idx]\n",
    "#print sess.run(tf.gather(b,idx))\n",
    "print sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_xs, _ , batch_idx = mnist.train.next_batch(3)\n",
    "batch_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for var in tf.trainable_variables():\n",
    "#    print var.name\n",
    "np.amax(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
